{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:21:41.641396Z",
     "start_time": "2024-09-25T16:21:41.635519Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Hello World!\")",
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1f86db3dc271817f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:21:59.111888Z",
     "start_time": "2024-09-25T16:21:59.104233Z"
    }
   },
   "cell_type": "code",
   "source": "1+2",
   "id": "bf42f30322dc34e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:24:14.113704Z",
     "start_time": "2024-09-25T16:24:14.108635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def mult(x, y):\n",
    "    return x * y\n",
    "print(mult(5, 10))"
   ],
   "id": "f3dd99d6dc559f6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:25:02.928353Z",
     "start_time": "2024-09-25T16:25:02.922842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Student:\n",
    "    def __init__(self, name, age, student_id):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "        self.student_id = student_id\n",
    "\n",
    "    def display_info(self):\n",
    "        return f\"Name: {self.name}, Age: {self.age}, Student ID: {self.student_id}\"\n",
    "\n",
    "# Example usage:\n",
    "student1 = Student(\"Alice\", 20, \"S12345\")\n",
    "print(student1.display_info())"
   ],
   "id": "4c4bc86cc9473746",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Alice, Age: 20, Student ID: S12345\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:32:56.814471Z",
     "start_time": "2024-09-25T16:32:56.811116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "sample = '{\"name\": \"Alice\", \"age\": 20}'\n",
    "print(json.loads(sample))"
   ],
   "id": "71c2cc71a1adb60",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Alice', 'age': 20}\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:35:53.744806Z",
     "start_time": "2024-09-25T16:35:53.738768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(type(sample))\n",
    "print(type(3.5))"
   ],
   "id": "b252569f32a16763",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:38:06.485429Z",
     "start_time": "2024-09-25T16:38:06.476996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class A:\n",
    "    def __init__(self):\n",
    "        self.__x = 1\n",
    "    \n",
    "    def get_x(self):\n",
    "        return self.__x\n",
    "\n",
    "a = A()\n",
    "print(a.get_x())"
   ],
   "id": "d297770ce8703808",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:40:32.817369Z",
     "start_time": "2024-09-25T16:40:32.810005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cars = [\"BMW\", \"Volvo\"]\n",
    "for car in cars:\n",
    "    print(car)"
   ],
   "id": "21671058fc7fec57",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BMW\n",
      "Volvo\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:43:11.132269Z",
     "start_time": "2024-09-25T16:43:11.127684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "is_sunny = True\n",
    "has_umbrella = False\n",
    "\n",
    "if is_sunny and has_umbrella:\n",
    "    print(\"I will go for a walk.\")\n",
    "elif is_sunny and not has_umbrella:\n",
    "    print(\"I will stay at home.\")\n",
    "elif not is_sunny and has_umbrella:\n",
    "    print(\"I will go for a walk with an umbrella.\")\n",
    "else:\n",
    "    print(\"I will stay at home.\")"
   ],
   "id": "353af51fcd7e57c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will stay at home.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T16:45:05.764738Z",
     "start_time": "2024-09-25T16:45:05.756319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "is_raining = not is_sunny\n",
    "if is_raining:\n",
    "    print(\"Bring an umbrella!\")\n",
    "\n",
    "needed_umbrella = is_raining and not has_umbrella\n",
    "if needed_umbrella:\n",
    "    print(\"Buy an umbrella!\")"
   ],
   "id": "b2b47e4843595653",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T19:31:27.217711Z",
     "start_time": "2024-09-25T19:25:08.219367Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install nltk scikit-learn",
   "id": "5a410f8d280efa1a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\r\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\r\n",
      "Collecting scikit-learn\r\n",
      "  Downloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\r\n",
      "Collecting click (from nltk)\r\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Collecting joblib (from nltk)\r\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\r\n",
      "Collecting regex>=2021.8.3 (from nltk)\r\n",
      "  Downloading regex-2024.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m40.5/40.5 kB\u001B[0m \u001B[31m389.2 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: tqdm in /home/justin/miniconda3/lib/python3.12/site-packages (from nltk) (4.65.0)\r\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/justin/miniconda3/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\r\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\r\n",
      "  Downloading scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m60.8/60.8 kB\u001B[0m \u001B[31m174.3 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting threadpoolctl>=3.1.0 (from scikit-learn)\r\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\r\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.5/1.5 MB\u001B[0m \u001B[31m474.8 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.9/12.9 MB\u001B[0m \u001B[31m131.3 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:03\u001B[0m\r\n",
      "\u001B[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m301.8/301.8 kB\u001B[0m \u001B[31m326.1 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading regex-2024.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (797 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m797.0/797.0 kB\u001B[0m \u001B[31m195.8 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.8 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m40.8/40.8 MB\u001B[0m \u001B[31m297.5 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:04\u001B[0m\r\n",
      "\u001B[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\r\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m97.9/97.9 kB\u001B[0m \u001B[31m418.3 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: threadpoolctl, scipy, regex, joblib, click, scikit-learn, nltk\r\n",
      "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.9.1 regex-2024.9.11 scikit-learn-1.5.2 scipy-1.14.1 threadpoolctl-3.5.0\r\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T19:49:44.528622Z",
     "start_time": "2024-09-25T19:49:05.811638Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install pandasql",
   "id": "503d1b48f2f5e998",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandasql\r\n",
      "  Downloading pandasql-0.7.3.tar.gz (26 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: numpy in /home/justin/miniconda3/lib/python3.12/site-packages (from pandasql) (1.26.4)\r\n",
      "Requirement already satisfied: pandas in /home/justin/miniconda3/lib/python3.12/site-packages (from pandasql) (2.2.2)\r\n",
      "Collecting sqlalchemy (from pandasql)\r\n",
      "  Downloading SQLAlchemy-2.0.35-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/justin/miniconda3/lib/python3.12/site-packages (from pandas->pandasql) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/justin/miniconda3/lib/python3.12/site-packages (from pandas->pandasql) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/justin/miniconda3/lib/python3.12/site-packages (from pandas->pandasql) (2023.3)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/justin/miniconda3/lib/python3.12/site-packages (from sqlalchemy->pandasql) (4.11.0)\r\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy->pandasql)\r\n",
      "  Downloading greenlet-3.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\r\n",
      "Requirement already satisfied: six>=1.5 in /home/justin/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->pandasql) (1.16.0)\r\n",
      "Downloading SQLAlchemy-2.0.35-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.2/3.2 MB\u001B[0m \u001B[31m187.0 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading greenlet-3.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (613 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m613.1/613.1 kB\u001B[0m \u001B[31m266.1 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hBuilding wheels for collected packages: pandasql\r\n",
      "  Building wheel for pandasql (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for pandasql: filename=pandasql-0.7.3-py3-none-any.whl size=26771 sha256=81c05273e56c6391fd44832c60ea7295598a22f5595f68501a5d07a344b6ac4b\r\n",
      "  Stored in directory: /home/justin/.cache/pip/wheels/15/a1/e7/6f92f295b5272ae5c02365e6b8fa19cb93f16a537090a1cf27\r\n",
      "Successfully built pandasql\r\n",
      "Installing collected packages: greenlet, sqlalchemy, pandasql\r\n",
      "Successfully installed greenlet-3.1.1 pandasql-0.7.3 sqlalchemy-2.0.35\r\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T21:05:28.299631Z",
     "start_time": "2024-09-25T21:05:28.108231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pandasql as psql\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the csv file into DataFrame\n",
    "df = os.path.join(\"data\", \"movie.csv\")\n",
    "\n",
    "# query a movie using title\n",
    "\n",
    "\n",
    "df = pd.read_csv(df)\n",
    "print(df.head())"
   ],
   "id": "98e6dd47e0a495b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0      id                title  \\\n",
      "0           0  960258           Shark Bait   \n",
      "1           1   11067      Major League II   \n",
      "2           2   20856  Aliens in the Attic   \n",
      "3           3   14098             100 Feet   \n",
      "4           4   11398       The Art of War   \n",
      "\n",
      "                                            overview release_date  popularity  \\\n",
      "0  A group of friends enjoying a weekend steal a ...   2022-05-13      34.679   \n",
      "1  After losing in the ALCS the year before, the ...   1994-03-30      16.374   \n",
      "2  A group of kids must protect their vacation ho...   2009-07-30      20.437   \n",
      "3  After Marnie Watson kills her abusive husband ...   2008-07-22      18.927   \n",
      "4  Neil Shaw is both agent and weapon - a critica...   2000-08-25      12.022   \n",
      "\n",
      "   vote_average  vote_count  \n",
      "0         5.697         318  \n",
      "1         5.697         320  \n",
      "2         5.696         973  \n",
      "3         5.696         304  \n",
      "4         5.696         477  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/justin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T20:23:23.346898Z",
     "start_time": "2024-09-25T20:23:23.341393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Basic in Natural Language ToolKit\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "sentence = \"\"\"At eight o'clock on Thursday morning Arthur didn't feel very good.\"\"\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "print(tokens)\n",
    "print(tagged)"
   ],
   "id": "823d6b1bee110a20",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.']\n",
      "[('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'NN'), ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN'), ('Arthur', 'NNP'), ('did', 'VBD'), (\"n't\", 'RB'), ('feel', 'VB'), ('very', 'RB'), ('good', 'JJ'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/justin/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/justin/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T20:39:46.469367Z",
     "start_time": "2024-09-25T20:39:46.444670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Knowing alternative of a word\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Ensure word net data is downloaded\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Find synonyms for the hello\n",
    "synonyms = wn.synsets(\"single\")\n",
    "\n",
    "# Get the lemmas (Base Word)\n",
    "hello_synonyms = set()\n",
    "for synonym in synonyms:\n",
    "    for lemma in synonym.lemmas():\n",
    "        hello_synonyms.add(lemma.name())\n",
    "        \n",
    "print(hello_synonyms)"
   ],
   "id": "99b4025f30ad644e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bingle', 'ace', 'one', 'I', 'individual', 'unmarried', 'undivided', '1', 'unity', 'exclusive', 'single'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/justin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T04:57:22.183487Z",
     "start_time": "2024-09-26T04:57:17.575044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# From external source\n",
    "import requests\n",
    "\n",
    "# Making a request to Datamuse API to get synonyms for \"hello\"\n",
    "response = requests.get(\"https://api.datamuse.com/words\", params={\"ml\": \"piano\", \"max\": 8})\n",
    "synonyms = response.json()\n",
    "\n",
    "# Extracting the top 3 words\n",
    "top_3_hello_synonyms = [word[\"word\"] for word in synonyms[:8]]\n",
    "print(top_3_hello_synonyms)"
   ],
   "id": "b3aea34c52443772",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pianoforte', 'pianissimo', 'soft', 'softly', 'forte-piano', 'pianissimo assai', 'spinet', 'violin']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "193d873c5e2399ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T05:13:08.793441Z",
     "start_time": "2024-09-26T05:12:56.938884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import pandasql as psql\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "\n",
    "# Download the necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "mv = os.path.join(\"data\", \"movie.csv\")\n",
    "df = pd.read_csv(mv)\n",
    "\n",
    "\n",
    "# Alias the DataFrame to the table name 'df'\n",
    "local_dict = {'df': df}\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the 'overview' texts\n",
    "vectors = vectorizer.fit_transform(df['overview'].astype(str))\n",
    "\n",
    "\n",
    "# Function to query using SQL\n",
    "def query(sql_query, local_dict):\n",
    "    return psql.sqldf(sql_query, local_dict)\n",
    "\n",
    "\n",
    "def find_similar_words(word, top_n=3):\n",
    "    # Tokenize all overviews into individual words\n",
    "    all_words = ' '.join(df['overview'].astype(str)).split()\n",
    "\n",
    "    # Get unique words\n",
    "    unique_words = set(all_words)\n",
    "\n",
    "    # Combine with the input word\n",
    "    vocabulary = list(unique_words.union({word}))\n",
    "\n",
    "    # Fit and transform the new vocabulary\n",
    "    extended_tf_vectors = vectorizer.fit_transform(vocabulary)\n",
    "    word_index = vocabulary.index(word)\n",
    "\n",
    "    # Calculate the cosine similarities of the input word with all other words\n",
    "    similarities = cosine_similarity(extended_tf_vectors[word_index], extended_tf_vectors).flatten()\n",
    "\n",
    "    # Get top N most similar words indices\n",
    "    similar_indices = similarities.argsort()[-(top_n + 1):-1][::-1]  # Exclude the word itself\n",
    "\n",
    "    # Return top N most similar words\n",
    "    return [vocabulary[i] for i in similar_indices]\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Prompt the user for a search keyword\n",
    "    keyword = input(\"Enter a keyword to search in the 'overview' column (or type 'exit' to quit): \")\n",
    "\n",
    "    if keyword.lower() == 'exit':\n",
    "        break\n",
    "\n",
    "    # Construct the SQL query\n",
    "    sql_query = f\"SELECT * FROM df WHERE overview LIKE '%{keyword}%'\"\n",
    "\n",
    "    # Execute the query\n",
    "    result = query(sql_query, local_dict)\n",
    "\n",
    "    if not result.empty:\n",
    "        print(result)\n",
    "    else:\n",
    "        print(\"No direct matches found. Searching for similar terms...\")\n",
    "        similar_words = find_similar_words(keyword)\n",
    "        \n",
    "        print(similar_words)\n",
    "\n",
    "        for similar_word in similar_words:\n",
    "            sql_query_similar = f\"SELECT * FROM df WHERE overview LIKE '%{similar_word}%'\"\n",
    "            result_similar = query(sql_query_similar, local_dict)\n",
    "\n",
    "            if not result_similar.empty:\n",
    "                print(f\"Results for similar word '{similar_word}':\")\n",
    "                print(result_similar)\n",
    "                break\n",
    "        else:\n",
    "            print(\"No matches found for similar words.\")\n",
    "\n",
    "print(\"Goodbye!\")"
   ],
   "id": "bda21575b2d1b9ca",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/justin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No direct matches found. Searching for similar terms...\n",
      "['love-and', 'Crime.', 'crime’,']\n",
      "Results for similar word 'love-and':\n",
      "   Unnamed: 0     id  title  \\\n",
      "0        7986  39053  Cyrus   \n",
      "\n",
      "                                            overview release_date  popularity  \\\n",
      "0  With John's social life at a standstill and hi...   2010-06-18      11.001   \n",
      "\n",
      "   vote_average  vote_count  \n",
      "0           5.9         399  \n",
      "Goodbye!\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T06:35:19.863759Z",
     "start_time": "2024-09-26T06:35:19.859285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.stem import SnowballStemmer # See which languages are supported\n",
    "print(\" \".join(SnowballStemmer.languages))"
   ],
   "id": "61f4c1dcc7dbdc35",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabic danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
